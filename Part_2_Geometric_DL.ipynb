{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: GNNs for Molecular Applications in Geometric Deep Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume:\n",
    "The second part of the course focuses on the application of GNNs to molecular data. We will explore different geometric deep learning approaches specifically designed for handling molecules, from small organic compounds to large biomolecules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "### 1. Introduction to Molecular Geometric Data\n",
    "- **Definition of Molecular Geometric Data:** Understand how molecular structures are represented as geometric data.\n",
    "- **Challenges in Molecular Data Processing:** Discuss challenges like irregularity, invariance to rotation/translation, and varying molecular size.\n",
    "\n",
    "### 2. Invariant Networks\n",
    "- **Concept of Invariance:** Learn the importance of invariant properties for molecular data.\n",
    "- **Key Architectures:** Overview of models like SchNet and Invariant Point Attention.\n",
    "- **Applications:** Use cases in molecular property prediction and drug discovery.\n",
    "\n",
    "### 3. Cartesian Networks\n",
    "- **Cartesian Coordinates in Molecular GNNs:** How GNNs use Cartesian data to model atomic interactions.\n",
    "- **Key Architectures:** Examples like GVP-GNN and E(n)-GNN.\n",
    "- **Applications:** Predicting molecular properties and dynamics.\n",
    "\n",
    "### 4. Spherical Networks\n",
    "- **Introduction to Spherical Molecular Data:** Understand how data on spherical domains is used in molecular modeling.\n",
    "- **Spherical GNNs:** Introduction to architectures like Tensor Fields Networks.\n",
    "- **Applications:** Modeling molecular conformations and protein structures.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### References\n",
    "- Based on: [\"Survey of Geometric GNNs for 3D Atomic Systems\"](https://arxiv.org/pdf/2312.07511)\n",
    "\n",
    "![Geometric GNNs for 3D atomic system](https://miro.medium.com/v2/resize:fit:1400/1*AYsGjZhbdr701OndCvnfng.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Molecular Geometric Data\n",
    "\n",
    "### Definition of Molecular Geometric Data\n",
    "Molecular structures are typically represented as point clouds where each point corresponds to an atom in 3D space. The geometric properties, such as atomic positions and bond lengths, are crucial for understanding molecular behavior and properties.\n",
    "\n",
    "### Challenges in Molecular Data Processing\n",
    "Processing molecular data involves several challenges:\n",
    "- **Irregularity:** Molecules can vary greatly in size and shape.\n",
    "- **Invariance to Rotation/Translation:** Molecular properties should not change under rotation or translation.\n",
    "- **Varying Molecular Size:** Models must handle small organic molecules and large proteins alike.\n",
    "\n",
    "### Applications in Molecular Data\n",
    "Molecular data is used in several key applications:\n",
    "- **Molecular Property Prediction:** Estimating properties like binding affinity, reactivity, or toxicity.\n",
    "- **Drug Discovery:** Identifying potential drug candidates by modeling molecule interactions.\n",
    "- **Protein Structure Prediction:** Determining 3D protein structures from amino acid sequences.\n",
    "\n",
    "### Molecules as Graphs\n",
    "Molecules can be represented as graphs where atoms are nodes and bonds are edges. This graph representation allows GNNs to learn complex molecular relationships and predict their properties accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Invariant Networks\n",
    "\n",
    "### Concept of Invariance\n",
    "Invariant networks in geometric deep learning ensure that molecular representations remain unchanged under transformations like rotations and translations. This invariance is crucial for molecular data, where the orientation or position of the molecule should not affect the prediction.\n",
    "\n",
    "### Key Architectures: SchNet and Invariant Point Attention\n",
    "\n",
    "#### SchNet\n",
    "**SchNet** is a neural network designed for predicting molecular properties using 3D geometries of molecules. It employs continuous-filter convolutional layers that operate on atomistic point clouds, making it invariant to rotations and translations.\n",
    "\n",
    "![SchNet Architecture](https://d3i71xaburhd42.cloudfront.net/5bf31dc4bd54b623008c13f8bc8954dc7c9a2d80/4-Figure2-1.png)\n",
    "\n",
    "**Paper Link:** [SchNet: A Continuous-filter Convolutional Neural Network for Modeling Quantum Interactions](https://arxiv.org/abs/1706.08566)\n",
    "\n",
    "#### Invariant Point Attention\n",
    "**Invariant Point Attention (IPA)** is a technique used in models like AlphaFold for processing geometric data, particularly for molecular modeling. IPA ensures the attention mechanism is invariant to 3D transformations, making it effective for tasks like predicting protein folding.\n",
    "\n",
    "![Invariant Point Attention](https://github.com/lucidrains/invariant-point-attention/blob/main/ipa.png?raw=true)\n",
    "\n",
    "**Paper Link:** [AlphaFold: Deep Learning-Based Protein Structure Prediction](https://www.nature.com/articles/s41586-021-03819-2)\n",
    "\n",
    "### Applications\n",
    "- **Molecular Property Prediction:** Estimating molecular properties such as binding affinity and reactivity.\n",
    "- **Drug Discovery:** Identifying new drug candidates by modeling molecular interactions.\n",
    "\n",
    "### Implementing an Invariant Network with `MessagePassing`\n",
    "\n",
    "Below is a simple example of an invariant network using the `MessagePassing` class from PyTorch Geometric:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class InvariantMPNN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, num_rbf=16):\n",
    "        super(InvariantMPNN, self).__init__(aggr='add')\n",
    "        self.lin = Linear(in_channels, out_channels)\n",
    "        self.dist_lin = Linear(num_rbf, out_channels)  # Linear transformation for RBF-transformed distances\n",
    "        self.rbf_centers = torch.linspace(0, 5, num_rbf)  # Radial basis function centers\n",
    "        self.rbf_gamma = torch.tensor(1.0)  # Gamma parameter for RBF\n",
    "\n",
    "    def forward(self, x, pos, edge_index):\n",
    "        # x: Node features\n",
    "        # pos: Node coordinates\n",
    "        # edge_index: Edge indices\n",
    "\n",
    "        # Calculate distances between connected nodes\n",
    "        row, col = edge_index\n",
    "        edge_vectors = pos[row] - pos[col]\n",
    "        distances = torch.norm(edge_vectors, p=2, dim=-1).unsqueeze(-1)\n",
    "        \n",
    "        # Compute RBF of distances\n",
    "        rbf = torch.exp(-self.rbf_gamma * (distances - self.rbf_centers) ** 2)\n",
    "\n",
    "        # Propagate messages\n",
    "        return self.propagate(edge_index, x=x, rbf=rbf)\n",
    "\n",
    "    def message(self, x_j, rbf):\n",
    "        # x_j: Source node features\n",
    "        # rbf: RBF-transformed distance features\n",
    "\n",
    "        edge_features = self.dist_lin(rbf)  # Transform RBF features\n",
    "        return self.lin(x_j) + edge_features  # Combine node and edge features\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out: Aggregated messages\n",
    "        return F.relu(aggr_out)  # Apply ReLU non-linearity\n",
    "\n",
    "# Example usage\n",
    "node_features = torch.randn(4, 3)  # 4 nodes, 3 features per node\n",
    "node_coords = torch.randn(4, 3)  # 4 nodes, 3D coordinates\n",
    "edge_index = torch.tensor([[0, 1, 2, 3], [1, 2, 3, 0]], dtype=torch.long)  # Edges in COO format\n",
    "\n",
    "model = InvariantMPNN(in_channels=3, out_channels=2)\n",
    "output = model(node_features, node_coords, edge_index)\n",
    "\n",
    "print(\"Output Node Features:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the Code:\n",
    "\n",
    "* **Distance Calculation:** Computes Euclidean distances between connected nodes.\n",
    "* **Radial Basis Function (RBF):** Transforms distances into edge features using RBF.\n",
    "* **Message Passing:** Uses transformed edge features along with node features for message passing.\n",
    "* **Update Function:** Applies ReLU activation to aggregated messages to update node features.\n",
    "\n",
    "This implementation makes the MPNN invariant to rotations and translations by using distance-based features, making it well-suited for molecular applications such as property prediction and drug discovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivariant Networks\n",
    "![Invariance vs Equivariance](https://i.sstatic.net/BdU1v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cartesian Networks\n",
    "\n",
    "### Cartesian Coordinates in Molecular GNNs\n",
    "\n",
    "Cartesian networks use Cartesian coordinates to model atomic interactions in molecules. Each atom's position in 3D space is represented by its Cartesian coordinates  $(x, y, z)$. These coordinates are used to compute distances and angles between atoms, which are fundamental for understanding molecular properties and dynamics.\n",
    "\n",
    "\n",
    "he main idea behind Cartesian networks is to use weighted sums of vectors that are equivariant to rotations of the input data (e.g., vectors between atoms or hidden vector fields), where the weights are learned from the data. \n",
    "\n",
    "### Key Architectures: GVP-GNN and E(n)-GNN\n",
    "\n",
    "#### 1. **Geometric Vector Perceptron (GVP-GNN)**\n",
    "The **GVP-GNN** is designed for molecular systems, integrating geometric information by using both scalar and vector features. It leverages vector operations to maintain rotational and translational equivariance, allowing it to predict molecular properties that depend on atomic positions.\n",
    "\n",
    "![GVP-GNN Architecture](https://raphael.tc.com/publication/gvp/featured_hu9b7018f8c7956abbafe4971f4d4d6c72_752084_720x0_resize_lanczos_2.png)\n",
    "\n",
    "- **Paper Link:** [Learning Protein Structure with a Differentiable Simulator](https://arxiv.org/abs/2009.01411)\n",
    "\n",
    "#### 2. **E(n)-Equivariant Graph Neural Networks (E(n)-GNN)**\n",
    "**E(n)-GNN** extends GNNs to be equivariant under Euclidean transformations (translations, rotations, and reflections). This network computes features that transform consistently under Euclidean transformations by using equivariant operations that process geometric data in a way consistent with physical laws.\n",
    "\n",
    "- **Key Idea:** E(n)-GNN maintains equivariance by incorporating operations that respect the symmetries of Euclidean space.\n",
    "\n",
    "![E(n)-GNN Architecture](https://ehoogeboom.github.io/publication/egnn/featured_hua4419112e0b0f9c21e721be460820b18_120982_680x500_fill_q90_lanczos_center_2.png)\n",
    "\n",
    "- **Paper Link:** [E(n) Equivariant Graph Neural Networks](https://arxiv.org/abs/2102.09844)\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **Predicting Molecular Properties:** GVP-GNN and E(n)-GNN are used for tasks such as predicting binding affinities, chemical reactivity, and electronic properties.\n",
    "- **Modeling Molecular Dynamics:** These architectures help simulate molecular motions and interactions over time, providing insights into complex molecular behaviors like folding and binding. \n",
    "\n",
    "### Mathematical Formulation of E(n)-GNN\n",
    "\n",
    "The E(n)-GNN updates the node features $h_i$ and coordinates $x_i$ for each node $i$ as follows:\n",
    "\n",
    "1. **Node Update:**\n",
    "$$\n",
    "h_i' = h_i + \\sum_{j \\in \\mathcal{N}(i)} f_{\\text{node}}\\left(h_i, h_j, ||x_i - x_j||\\right)\n",
    "$$\n",
    "where:\n",
    "- $h_i$ is the feature of node $i$.\n",
    "- $x_i$ is the coordinate of node $i$.\n",
    "- $\\mathcal{N}(i)$ is the set of neighbors of node $i$.\n",
    "- $f_{\\text{node}}$ is a learnable function.\n",
    "\n",
    "2. **Coordinate Update:**\n",
    "$$\n",
    "x_i' = x_i + \\frac{1}{| \\mathcal{N}(i) |} \\sum_{j \\in \\mathcal{N}(i)} (x_j - x_i) \\cdot g_{\\text{coord}}(h_i, h_j, ||x_i - x_j||)\n",
    "$$\n",
    "where:\n",
    "- $g_{\\text{coord}}$ is a learnable function.\n",
    "\n",
    "### Code Snippet: Implementing a Simple Cartesian Network\n",
    "\n",
    "Here is a simple implementation of a Cartesian-based GNN layer using PyTorch Geometric:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "class CartesianGNNLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CartesianGNNLayer, self).__init__(aggr='add')  # Aggregation function: 'add'\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * in_channels + 1, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        self.coord_mlp = nn.Sequential(\n",
    "            nn.Linear(out_channels, 1),\n",
    "            nn.Tanh()  # To limit coordinate updates\n",
    "        )\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(in_channels + out_channels, out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, pos, edge_index):\n",
    "        num_nodes = x.size(0)\n",
    "        # Start message passing\n",
    "        out = self.propagate(edge_index, x=x, pos=pos, size=(num_nodes, num_nodes))\n",
    "        x_out, coord_updates = out\n",
    "        # Update positions\n",
    "        pos = pos + coord_updates\n",
    "        return x_out, pos\n",
    "    \n",
    "    def message(self, x_i, x_j, pos_i, pos_j):\n",
    "        # Relative positional differences and distances\n",
    "        diff = pos_i - pos_j  # [num_edges, 3]\n",
    "        dist = torch.norm(diff, dim=-1, keepdim=True)  # [num_edges, 1]\n",
    "        \n",
    "        # Edge features: concatenate node features and distance\n",
    "        edge_input = torch.cat([x_i, x_j, dist], dim=-1)  # [num_edges, 2 * in_channels + 1]\n",
    "        e_ij = self.edge_mlp(edge_input)  # [num_edges, out_channels]\n",
    "        \n",
    "        # Compute coordinate updates\n",
    "        coord_update = self.coord_mlp(e_ij) * diff  # [num_edges, 3]\n",
    "        \n",
    "        return e_ij, coord_update\n",
    "    \n",
    "    def aggregate(self, inputs, index, ptr=None, dim_size=None):\n",
    "        e_ij, coord_update = inputs\n",
    "        num_nodes = dim_size  # Total number of nodes\n",
    "        # Aggregate edge features\n",
    "        aggr_e = scatter(e_ij, index, dim=0, dim_size=num_nodes, reduce='add')\n",
    "        # Aggregate coordinate updates\n",
    "        aggr_coord = scatter(coord_update, index, dim=0, dim_size=num_nodes, reduce='mean')\n",
    "        return aggr_e, aggr_coord\n",
    "    \n",
    "    def update(self, aggr_out, x):\n",
    "        aggr_e, aggr_coord = aggr_out\n",
    "        # Update node features\n",
    "        node_input = torch.cat([x, aggr_e], dim=-1)  # Concatenate along feature dimension\n",
    "        x_out = self.node_mlp(node_input)\n",
    "        return x_out, aggr_coord\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(10, 3)  # 10 nodes, 3 features per node\n",
    "pos = torch.randn(10, 3)  # 10 nodes, 3D positions\n",
    "edge_index = torch.tensor([[0, 1, 2], [1, 2, 0]], dtype=torch.long)  # Edge index\n",
    "\n",
    "layer = CartesianGNNLayer(in_channels=3, out_channels=2)\n",
    "out = layer(x, pos, edge_index)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spherical Networks\n",
    "\n",
    "### Higher-Degree Representations in Molecular GNNs\n",
    "\n",
    "In Cartesian networks, we operate primarily on **1-degree features** (vectors), which are sufficient to describe single elements in space. However, molecular structures often exhibit complex geometric relationships that cannot be fully captured by vectors alone. **Spherical networks** address this limitation by utilizing representations capable of expressing **higher-degree features**. These higher-degree features can capture intricate spatial patterns and symmetries essential for accurately modeling molecular interactions.\n",
    "\n",
    "### Separating Radial and Angular Components\n",
    "\n",
    "To achieve precise representations of molecular data, spherical networks consider functions defined in 3D space with separate **radial** and **angular** components. The **radial part** requires high precision to accurately represent distances between atoms, while the **angular part** captures the orientations and directions of atomic interactions. By treating these components separately, spherical networks can model complex molecular interactions more effectively.\n",
    "\n",
    "### Spherical Harmonics as Fourier Basis on a Sphere\n",
    "\n",
    "**Spherical harmonics** serve as the Fourier basis functions on a sphere and are fundamental in representing the angular component of functions in 3D space. They allow us to decompose complex angular dependencies into a series of functions with varying degrees $ l $, each corresponding to different levels of detail and symmetry properties.\n",
    "\n",
    "- **Degrees $ l $**: Represent different levels of angular frequency, with higher degrees capturing more complex angular variations.\n",
    "- **Orders $ m $**: Range from $-l$ to $+l$ and represent different orientations for a given degree.\n",
    "\n",
    "### Irreducible Representations (Irreps) of Different Degrees\n",
    "\n",
    "1. **0-Degree Irreps (Scalars)**: Invariant under rotation, representing features that do not change with orientation.\n",
    "2. **1-Degree Irreps (Vectors)**: Transform linearly under rotations, representing directional features.\n",
    "3. **Higher-Degree Irreps**: Degrees $ l = 2, 3, \\ldots $ correspond to more complex features (e.g., tensors) that capture higher-order geometric information and transform predictably under rotations.\n",
    "\n",
    "### Message Passing in Spherical Networks\n",
    "\n",
    "In spherical GNNs, the message-passing mechanism involves projecting features onto spherical harmonics and handling higher-degree interactions.\n",
    "\n",
    "#### 1. Feature Functions\n",
    "\n",
    "Each node $ j $ has a feature function $ f_j(\\vec{r}) $ that can be expanded in terms of spherical harmonics of degree $ l_1 $:\n",
    "\n",
    "$$\n",
    "f_j(\\vec{r}) = \\sum_{l_1, m_1} f_j^{l_1 m_1} Y_{l_1}^{m_1}(\\hat{r})\n",
    "$$\n",
    "\n",
    "- $ f_j^{l_1 m_1} $: Coefficients for the spherical harmonic components.\n",
    "- $ Y_{l_1}^{m_1}(\\hat{r}) $: Spherical harmonics evaluated at direction $ \\hat{r} $.\n",
    "\n",
    "#### 2. Delta Function Expansion\n",
    "\n",
    "The delta function $ \\delta(\\vec{r}_{ij}) $, representing the positional difference between nodes $ i $ and $ j $, is expanded in spherical harmonics of degree $ l_2 $:\n",
    "\n",
    "$$\n",
    "\\delta(\\vec{r}_{ij}) = \\delta(r_{ij}) \\sum_{l_2, m_2} Y_{l_2}^{m_2}(\\hat{r}_{ij})\n",
    "$$\n",
    "\n",
    "- $ \\delta(r_{ij}) $: Radial part ensuring precision in distances.\n",
    "- $ \\hat{r}_{ij} $: Unit vector in the direction from $ i $ to $ j $.\n",
    "\n",
    "#### 3. Tensor Product and Projection\n",
    "\n",
    "The product of the feature function $ f_j(\\vec{r}) $ and the delta function $ \\delta(\\vec{r}_{ij}) $ is projected onto degree $ l_3 $ using the **Clebsch-Gordan coefficients**, resulting in message terms:\n",
    "\n",
    "$$\n",
    "[m_{ij}]^{l_3 m_3}_{l_1 l_2} = \\sum_{m_1, m_2} \\langle l_1 m_1, l_2 m_2 | l_3 m_3 \\rangle f_j^{l_1 m_1} Y_{l_2}^{m_2}(\\hat{r}_{ij})\n",
    "$$\n",
    "\n",
    "- $ \\langle l_1 m_1, l_2 m_2 | l_3 m_3 \\rangle $: Clebsch-Gordan coefficients for coupling degrees $ l_1 $ and $ l_2 $ to $ l_3 $.\n",
    "- This operation ensures the resulting features transform correctly under rotations.\n",
    "\n",
    "#### 4. Message Computation\n",
    "\n",
    "The message from node $ j $ to node $ i $ is computed by summing over all combinations of degrees $ l_1, l_2, l_3 $:\n",
    "\n",
    "$$\n",
    "m_{ij} = \\sum_{l_1, l_2, l_3} w^{l_3}_{l_1 l_2} [m_{ij}]^{l_3}_{l_1 l_2}\n",
    "$$\n",
    "\n",
    "- $ w^{l_3}_{l_1 l_2} $: Learnable weights obtained from data, capturing interaction strengths.\n",
    "- The summation over degrees allows the network to capture interactions at multiple levels of complexity.\n",
    "\n",
    "#### 5. Feature Update\n",
    "\n",
    "Each node $ i $ updates its features by aggregating the messages from its neighbors:\n",
    "\n",
    "$$\n",
    "f_i' = \\sum_{j \\in \\mathcal{N}(i)} m_{ij}\n",
    "$$\n",
    "\n",
    "- $ \\mathcal{N}(i) $: Set of neighboring nodes of $ i $.\n",
    "- This aggregation maintains the equivariance property of the network.\n",
    "\n",
    "### Tensor Product and Clebsch-Gordan Coefficients\n",
    "\n",
    "The **tensor product** $ \\otimes $ combines two irreducible representations of degrees $ l_1 $ and $ l_2 $ to produce representations of degree $ l_3 $. The Clebsch-Gordan coefficients govern this coupling, ensuring that the resulting features transform correctly under rotations, thus preserving **equivariance**.\n",
    "\n",
    "### Example: Tensor Field Networks (TFNs)\n",
    "\n",
    "**Tensor Field Networks (TFNs)** exemplify spherical networks that maintain equivariance under rotations and translations by operating on higher-degree irreps.\n",
    "\n",
    "- **Feature Transformation**: Atomic features are expanded into spherical harmonics, capturing complex angular dependencies.\n",
    "- **Equivariant Convolution**: TFNs perform convolutions using tensor products and Clebsch-Gordan coefficients, combining features in a rotation-equivariant manner.\n",
    "- **Feature Aggregation**: Messages are aggregated over neighbors, with higher-degree irreps enabling the network to model intricate spatial relationships.\n",
    "\n",
    "#### Mathematical Formulation in TFNs\n",
    "\n",
    "1. **Input Features**: $ f_j^{l m} $ for each node $ j $.\n",
    "2. **Message Passing**:\n",
    "\n",
    "   $$\n",
    "   m_{ij}^{l_3 m_3} = \\sum_{l_1, l_2} \\sum_{m_1, m_2} w^{l_3}_{l_1 l_2} \\langle l_1 m_1, l_2 m_2 | l_3 m_3 \\rangle f_j^{l_1 m_1} Y_{l_2}^{m_2}(\\hat{r}_{ij}) \\delta(r_{ij})\n",
    "   $$\n",
    "\n",
    "3. **Feature Update**:\n",
    "\n",
    "   $$\n",
    "   f_i'^{l_3 m_3} = \\sum_{j \\in \\mathcal{N}(i)} m_{ij}^{l_3 m_3}\n",
    "   $$\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **Modeling Molecular Conformations**: By capturing higher-order geometric features, spherical networks like TFNs can predict molecular shapes and dynamics with high accuracy.\n",
    "- **Protein Structure Prediction**: The ability to model complex interactions and orientations makes spherical networks suitable for predicting 3D protein structures.\n",
    "\n",
    "### Visualization of Spherical Harmonics\n",
    "\n",
    "![Spherical Harmonics Visualization](https://upload.wikimedia.org/wikipedia/commons/7/74/Real_Spherical_Harmonics_Figure_Table_Complex_Radial_Magnitude.gif)\n",
    "\n",
    "- The images represent spherical harmonics of various degrees and orders, visualizing how higher-degree functions capture more complex angular patterns.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Higher-Degree Features**: Spherical networks extend beyond vectors to include higher-degree features, enabling richer representations of molecular structures.\n",
    "- **Separation of Radial and Angular Components**: Handling radial and angular components separately allows for precise modeling of distances and orientations.\n",
    "- **Equivariance through Spherical Harmonics**: Using spherical harmonics and tensor products ensures that network operations are equivariant under rotations, crucial for modeling physical systems.\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **Tensor Field Networks Paper**: [Tensor Field Networks: Rotation- and Translation-Equivariant Neural Networks for 3D Point Clouds](https://arxiv.org/abs/1802.08219)\n",
    "- **Clebsch-Gordan Coefficients Tutorial**: [Understanding Clebsch-Gordan Coefficients](https://quantummechanics.ucsd.edu/ph130a/130_notes/node328.html)\n",
    "\n",
    "\n",
    "![Tensor Field FLow](https://github.com/RobDHess/Steerable-E3-GNN/raw/main/assets/forward_pass_faster_larger.gif)\n",
    "![TFN illustration](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41467-022-29939-5/MediaObjects/41467_2022_29939_Fig1_HTML.png?as=webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of a Simple Spherical GNN Layer\n",
    "In this section, we will implement a basic spherical GNN layer. This layer computes radial basis functions (RBF) of distances between nodes and spherical harmonics of the angular part of the vectors between nodes. The input degree is 0, and the output degree is 2.\n",
    "\n",
    "We will use predefined functions to compute spherical harmonics and tensor products, and we will load Clebsch-Gordan coefficients to handle the operations correctly.\n",
    "\n",
    "Step-by-Step Implementation\n",
    "1. Define spherical harmonics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def associated_legendre_polynomials(L, x):\n",
    "    \"\"\"\n",
    "    Compute the associated Legendre polynomials.\n",
    "\n",
    "    Parameters:\n",
    "    L (int): The maximum degree of the polynomials.\n",
    "    x (torch.Tensor): The input tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A tensor containing the associated Legendre polynomials.\n",
    "    \"\"\"\n",
    "    P = [torch.ones_like(x) for _ in range((L+1)*L//2)]\n",
    "    \n",
    "    # Compute the polynomials for l in range(1, L)\n",
    "    for l in range(1, L):\n",
    "        P[(l+3)*l//2] = - np.sqrt((2*l-1)/(2*l)) * torch.sqrt(1-x**2) * P[(l+2)*(l-1)//2]\n",
    "    \n",
    "    # Compute the polynomials for m in range(L-1)\n",
    "    for m in range(L-1):\n",
    "        P[(m+2)*(m+1)//2+m] = x * np.sqrt(2*m+1) * P[(m+1)*m//2+m]\n",
    "        for l in range(m+2, L):\n",
    "            P[(l+1)*l//2+m] = ((2*l-1)*x*P[l*(l-1)//2 + m]/np.sqrt((l**2-m**2)) - P[(l-1)*(l-2)//2+m]*np.sqrt(((l-1)**2-m**2)/(l**2-m**2)))\n",
    "    return torch.stack(P, dim=0)\n",
    "\n",
    "def spherical_harmonics(L, THETA, PHI, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    \"\"\"\n",
    "    Compute the spherical harmonics.\n",
    "\n",
    "    Parameters:\n",
    "    L (int): The maximum degree of the harmonics.\n",
    "    THETA (torch.Tensor): The theta angles.\n",
    "    PHI (torch.Tensor): The phi angles.\n",
    "    device (torch.device): The device to use for computations (default is CUDA if available).\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tensors containing the spherical harmonics for each degree l.\n",
    "    \"\"\"\n",
    "    P = associated_legendre_polynomials(L, torch.cos(PHI))\n",
    "    M2 =  [torch.zeros_like(THETA) for _ in range(2*(L-1)+1)]\n",
    "    output =  [[torch.zeros_like(THETA, device = device) for _ in range(2*l+1)] for l in range(L)]\n",
    "    \n",
    "    # Compute cosine and sine components for each m\n",
    "    for m in range(L):\n",
    "        if m > 0:\n",
    "            M2[L-1+m] = torch.cos(m*THETA)\n",
    "            M2[L-1-m] = torch.sin(m*THETA)\n",
    "        else:\n",
    "            M2[L-1]  = torch.ones_like(THETA)\n",
    "    \n",
    "    # Compute the spherical harmonics for each l and m\n",
    "    for l in range(L):\n",
    "        for m in range(l+1):\n",
    "            if m > 0:\n",
    "                output[l][l+m] = np.sqrt(2)*P[(l+1)*l//2+m]*np.sqrt((2*l+1)/(4*np.pi))*M2[L-1+m]\n",
    "                output[l][l-m] = np.sqrt(2)*P[(l+1)*l//2+m]*np.sqrt((2*l+1)/(4*np.pi))*M2[L-1-m]\n",
    "            else:\n",
    "                output[l][l  ] = P[(l+1)*l//2]*np.sqrt((2*l+1)/(4*np.pi))*M2[L-1]\n",
    "    \n",
    "    return torch.concat([torch.stack(output_i, dim = 0).to(device) for output_i in output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement the tensor product. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_product(f_j, Y_r, cg, W, rbf, in_degree, r_degree, out_degree):\n",
    "    # Tensor product using Clebsch-Gordan coefficients\n",
    "    in_degree_to_order = torch.tensor([int(np.floor(np.sqrt(i + 1)))-1 for i in range((in_degree + 1) ** 2)], dtype=torch.long)\n",
    "    r_degree_to_order = torch.tensor([int(np.floor(np.sqrt(i + 1)))-1 for i in range((r_degree + 1) ** 2)], dtype=torch.long)\n",
    "    out_degree_to_order = torch.tensor([int(np.floor(np.sqrt(i + 1)))-1 for i in range((out_degree + 1) ** 2)], dtype=torch.long)\n",
    "    # print(W.shape, in_degree_to_order.shape, r_degree_to_order.shape, out_degree_to_order.shape)\n",
    "    W_spanned = ((W[in_degree_to_order])[:, r_degree_to_order])[:, :, out_degree_to_order]\n",
    "    # print(cg.shape,  in_degree, r_degree, out_degree)\n",
    "    # print( f_j.shape)\n",
    "    # print(Y_r.shape)\n",
    "    # print((cg[:(in_degree + 1) ** 2, :(r_degree + 1) ** 2, :(out_degree + 1) ** 2, ]).shape)\n",
    "    # print(W_spanned.shape)\n",
    "    # print(rbf.shape)\n",
    "    out = torch.einsum('exa, ye, xyz, xyzabr, er->ezb', f_j, Y_r, cg[:(in_degree + 1) ** 2, :(r_degree + 1) ** 2, :(out_degree + 1) ** 2, ], W_spanned, rbf)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define a simple Spherical GNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.functional import softplus\n",
    "\n",
    "# Define the Spherical GNN Layer\n",
    "class SphericalGNNLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, num_rbf=16, in_degree=0, r_degree=2, out_degree=2):\n",
    "        super(SphericalGNNLayer, self).__init__(aggr='add')\n",
    "        self.num_rbf = num_rbf\n",
    "        self.in_degree = in_degree\n",
    "        self.r_degree = r_degree\n",
    "        self.out_degree = out_degree\n",
    "        self.rbf_centers = torch.linspace(0, 5, num_rbf)  # Radial basis function centers\n",
    "        self.rbf_gamma = torch.tensor(1.0)  # Gamma parameter for RBF\n",
    "\n",
    "        \n",
    "        self.W = nn.Parameter(torch.randn( self.in_degree+1, self.r_degree+1, self.out_degree+1, in_channels, out_channels, self.num_rbf))\n",
    "\n",
    "        # Load Clebsch-Gordan coefficients\n",
    "        self.cg = torch.load('CG_tensor_2.pt')\n",
    "\n",
    "    def forward(self, x, pos, edge_index):\n",
    "        # Compute pairwise distances\n",
    "        row, col = edge_index\n",
    "        diff = pos[row] - pos[col]\n",
    "        dist = diff.norm(dim=-1)\n",
    "\n",
    "        # Compute RBF and spherical harmonics\n",
    "        rbf = torch.exp(-self.rbf_gamma[None] * (dist[:, None] - self.rbf_centers[None]) ** 2)\n",
    "        sh = self.spherical_harmonics(diff)\n",
    "\n",
    "        # Perform message passing\n",
    "        out = self.propagate(edge_index, x=x, rbf=rbf, sh=sh)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, rbf, sh):\n",
    "        # Tensor product of input features with spherical \n",
    "        x_j  = torch.reshape(x_j, (x_j.shape[0], (self.in_degree+1)**2, -1))\n",
    "        tp = tensor_product(x_j, sh, self.cg, self.W, rbf, self.in_degree, self.r_degree, self.out_degree)\n",
    "        return tp.reshape(tp.shape[0], -1)\n",
    "\n",
    "    \n",
    "\n",
    "    def spherical_harmonics(self, vectors):\n",
    "        # Compute spherical harmonics of vectors\n",
    "        theta = torch.atan2(vectors[:, 1], vectors[:, 0])\n",
    "        phi = torch.acos(vectors[:, 2] / vectors.norm(dim=-1))\n",
    "        sh = spherical_harmonics(self.r_degree + 1, theta, phi)\n",
    "        return sh\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "node_features = torch.randn(4, 3)  # 4 nodes, 3 features per node\n",
    "node_coords = torch.randn(4, 3)  # 4 nodes, 3D coordinates\n",
    "edge_index = torch.tensor([[0, 1, 2, 3], [1, 2, 3, 0]], dtype=torch.long)  # Edges in COO format\n",
    "\n",
    "model = SphericalGNNLayer(in_channels=3, out_channels=2)\n",
    "output = model(node_features, node_coords, edge_index)\n",
    "\n",
    "print(\"Output Node Features:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the Key Components\n",
    "\n",
    "1. **Radial Basis Functions (RBF):** A set of functions applied to the distances between nodes. The output is transformed via a linear layer and activation function to capture the radial dependencies.\n",
    "\n",
    "2. **Spherical Harmonics (SH):** Computed from the angular parts of vectors between nodes. SH helps encode the angular information of atomic positions.\n",
    "\n",
    "3. **Tensor Product:** Combines the input features with the spherical harmonics using Clebsch-Gordan coefficients to produce higher-degree features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Training a GNN on the QM9 Dataset using Custom Layers\n",
    "\n",
    "In this final exercise, you will train a GNN on the `QM9` dataset using one of the custom layers you have implemented: `InvariantMPNN`, `CartesianGNNLayer`, or `SphericalGNNLayer`.\n",
    "\n",
    "### Step 1: Load the QM9 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Load the QM9 dataset\n",
    "dataset = QM9(root='data/QM9')\n",
    "\n",
    "\n",
    "\n",
    "# choose the target\n",
    "\n",
    "\n",
    "'''\n",
    "+--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | Target | Property                         | Description                                                                       | Unit                                        |\n",
    "    +========+==================================+===================================================================================+=============================================+\n",
    "    | 0      | :math:`\\mu`                      | Dipole moment                                                                     | :math:`\\textrm{D}`                          |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 1      | :math:`\\alpha`                   | Isotropic polarizability                                                          | :math:`{a_0}^3`                             |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 2      | :math:`\\epsilon_{\\textrm{HOMO}}` | Highest occupied molecular orbital energy                                         | :math:`\\textrm{eV}`                         |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 3      | :math:`\\epsilon_{\\textrm{LUMO}}` | Lowest unoccupied molecular orbital energy                                        | :math:`\\textrm{eV}`                         |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 4      | :math:`\\Delta \\epsilon`          | Gap between :math:`\\epsilon_{\\textrm{HOMO}}` and :math:`\\epsilon_{\\textrm{LUMO}}` | :math:`\\textrm{eV}`                         |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 5      | :math:`\\langle R^2 \\rangle`      | Electronic spatial extent                                                         | :math:`{a_0}^2`                             |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 6      | :math:`\\textrm{ZPVE}`            | Zero point vibrational energy                                                     | :math:`\\textrm{eV}`                         |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 7      | :math:`U_0`                      | Internal energy at 0K                                                             | :math:`\\textrm{eV}`                         |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 8      | :math:`U`                        | Internal energy at 298.15K                                                        | :math:`\\textrm{eV}`                         |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 9      | :math:`H`                        | Enthalpy at 298.15K                                                               | :math:`\\textrm{eV}`                         |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 10     | :math:`G`                        | Free energy at 298.15K                                                            | :math:`\\textrm{eV}`                         |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 11     | :math:`c_{\\textrm{v}}`           | Heat capavity at 298.15K                                                          | :math:`\\frac{\\textrm{cal}}{\\textrm{mol K}}` |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 12     | :math:`U_0^{\\textrm{ATOM}}`      | Atomization energy at 0K                                                          | :math:`\\textrm{eV}`                         |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 13     | :math:`U^{\\textrm{ATOM}}`        | Atomization energy at 298.15K                                                     | :math:`\\textrm{eV}`                         |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 14     | :math:`H^{\\textrm{ATOM}}`        | Atomization enthalpy at 298.15K                                                   | :math:`\\textrm{eV}`                         |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 15     | :math:`G^{\\textrm{ATOM}}`        | Atomization free energy at 298.15K                                                | :math:`\\textrm{eV}`                         |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 16     | :math:`A`                        | Rotational constant                                                               | :math:`\\textrm{GHz}`                        |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 17     | :math:`B`                        | Rotational constant                                                               | :math:`\\textrm{GHz}`                        |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | 18     | :math:`C`                        | Rotational constant                                                               | :math:`\\textrm{GHz}`                        |\n",
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "\n",
    "    \n",
    "'''\n",
    "\n",
    "target = 0\n",
    "\n",
    "\n",
    "mean = dataset.data.y.mean(dim=0, keepdim=True)\n",
    "std = dataset.data.y.std(dim=0, keepdim=True)\n",
    "dataset.data.y = (dataset.data.y - mean) / std\n",
    "mean, std = mean[:, target].item(), std[:, target].item()\n",
    "\n",
    "\n",
    "# Splitting dataset...\n",
    "train_dataset = dataset[:110000]\n",
    "val_dataset = dataset[110000:120000]\n",
    "test_dataset = dataset[120000:]\n",
    "\n",
    "\n",
    "# DataLoader settings...\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Choose and Define the Model\n",
    "Select one of the custom layers (`InvariantMPNN`, `CartesianGNNLayer`, or `SphericalGNNLayer`) and define the model. Hereâ€™s an example using the `InvariantMPNN` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class CustomGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, layer_type='invariant'):\n",
    "        super(CustomGNN, self).__init__()\n",
    "        if layer_type == 'invariant':\n",
    "            self.conv1 = InvariantMPNN(in_channels, hidden_channels)\n",
    "        elif layer_type == 'cartesian':\n",
    "            self.conv1 = CartesianGNNLayer(in_channels, hidden_channels)\n",
    "        elif layer_type == 'spherical':\n",
    "            self.conv1 = SphericalGNNLayer(in_channels, hidden_channels)\n",
    "        \n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, pos, edge_index, batch):\n",
    "        # Message Passing Layer\n",
    "        x = self.conv1(x, pos, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # Global Pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = torch.relu(self.lin1(x))\n",
    "        return self.lin2(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = CustomGNN(in_channels=11, hidden_channels=64, out_channels=1, layer_type='invariant')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.pos, data.edge_index, data.batch)\n",
    "        loss = F.mse_loss(out, data.y[:, target])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    error = 0\n",
    "    for data in loader:\n",
    "        data = data.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x, data.pos, data.edge_index, data.batch)\n",
    "            error += (out - data.y[:, target]).abs().sum().item()\n",
    "    return error / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "target = 0  # Select the property index to predict\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    test_error = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Test MAE: {test_error:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
