{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: General Concepts of GNNs\n",
    "\n",
    "## Resume:\n",
    "\n",
    "The first part of this course focuses on the foundational concepts of Graph Neural Networks (GNNs). It covers the essential theoretical framework behind GNNs, including the structure of graphs, the process of message passing, and the algorithms used for graph isomorphism. We will learn about the core principles that enable GNNs to operate on graph data, understanding key techniques such as the Weisfeiler-Lehman test and various approaches to addressing the graph isomorphism problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: General Concepts of GNNs\n",
    "\n",
    "### 1. Introduction to Graphs\n",
    "- **Setup and Package Installation**\n",
    "- **Definition and Components:** Nodes, edges, adjacency matrices.\n",
    "- **Types of Graphs:** Directed vs. undirected, weighted vs. unweighted, cyclic vs. acyclic.\n",
    "- **Applications of Graphs:** Social networks, molecular structures, knowledge graphs.\n",
    "\n",
    "### 2. Message Passing in GNNs\n",
    "- **Basic Concept:** How information is passed between nodes in a graph.\n",
    "- **Message Function:** Types of messages, role in updating node representations.\n",
    "- **Aggregation Function:** Techniques for combining messages, common aggregation methods (e.g., sum, mean, max).\n",
    "- **Update Function:** How node states are updated based on aggregated messages.\n",
    "\n",
    "### 3. Message-Passing Neural Networks (MPNN)\n",
    "- **Architecture Overview:** Detailed look at how MPNNs operate.\n",
    "- **Variants of MPNNs:** GCN, GAT, GraphSAGE, etc.\n",
    "- **Training and Optimization:** Strategies for training GNNs, including loss functions and optimization techniques.\n",
    "\n",
    "### 4. Graph Isomorphism and Weisfeiler-Lehman Test\n",
    "- **Graph Isomorphism Problem:** Definition and significance in GNNs.\n",
    "- **Weisfeiler-Lehman Test:** Explanation of the test, how it is used to distinguish non-isomorphic graphs.\n",
    "- **Limitations and Extensions:** Discussing the limitations of the basic test and how GNNs attempt to overcome these.\n",
    "\n",
    "\n",
    "\n",
    "### 5. Approaches to the Isomorphism Problem in GNNs\n",
    "- **Expressive Power of GNNs:** Understanding the limits of GNNs in distinguishing different graphs.\n",
    "- **Advanced Techniques:** Higher-order GNNs, relational pooling, and other advanced methods to address graph isomorphism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs are fundamental structures used to model relationships between objects. A graph consists of nodes (or vertices) and edges that connect pairs of nodes. Here's how to set up your environment to work with graphs in Python.\n",
    "\n",
    "#### Installation of Necessary Packages\n",
    "\n",
    "Before we begin, ensure you have the necessary packages installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install networkx matplotlib torch torch-geometric numpy networkx collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition and Components\n",
    "\n",
    "A **graph** is a data structure that consists of two main components:\n",
    "- **Nodes (or vertices):** These are the entities or objects in the graph. They are often denoted by circles or points.\n",
    "- **Edges:** These represent the relationships or connections between the nodes. Edges can be directed (with a specific direction) or undirected (without direction), and can be weighted or unweighted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python Code: Creating and Visualizing a Simple Graph**\n",
    "Let's create a simple graph using the `networkx` library and visualize it using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a graph object\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "G.add_node(1)\n",
    "G.add_node(2)\n",
    "G.add_node(3)\n",
    "\n",
    "# Add edges\n",
    "G.add_edge(1, 2)\n",
    "G.add_edge(1, 3)\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(5,5))\n",
    "nx.draw(G, with_labels=True, node_size=700, node_color=\"lightblue\")\n",
    "plt.title(\"Simple Graph\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Graphs\n",
    "\n",
    "   **Directed Graph (Digraph):** Each edge has a direction, pointing from one node to another.\n",
    "   \n",
    "   ![Directed Graph](https://upload.wikimedia.org/wikipedia/commons/2/23/Directed_graph_no_background.svg)\n",
    "\n",
    "   \n",
    "\n",
    "   Here's how to create and visualize a directed graph:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directed graph\n",
    "D = nx.DiGraph()\n",
    "\n",
    "# Add nodes and directed edges\n",
    "D.add_edge(1, 2)\n",
    "D.add_edge(1, 3)\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(5,5))\n",
    "nx.draw(D, with_labels=True, node_size=700, node_color=\"lightblue\", arrowstyle=\"-|>\", arrowsize=20)\n",
    "plt.title(\"Directed Graph\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Undirected Graph:** Edges do not have a direction; they simply connect two nodes.\n",
    "\n",
    "   ![Undirected Graph](https://www.researchgate.net/profile/Stein-Malerud/publication/252675933/figure/fig1/AS:652972975476737@1532692297739/A-simple-undirected-graph-with-nodes-and-edges_W640.jpg)\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted Graph:** Each edge has a numerical value(s) (weight(s)) associated with it, representing the strength or capacity of the connection.\n",
    "\n",
    "   ![Weighted Graph](https://upload.wikimedia.org/wikipedia/commons/f/f0/Weighted_network.svg)\n",
    "\n",
    "\n",
    "Here's how to create and visualize a weighted graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weighted graph\n",
    "W = nx.Graph()\n",
    "\n",
    "# Add weighted edges\n",
    "W.add_edge(1, 2, weight=4.7)\n",
    "W.add_edge(1, 3, weight=3.2)\n",
    "\n",
    "# Draw the graph with edge labels\n",
    "pos = nx.spring_layout(W)  # positions for all nodes\n",
    "nx.draw(W, pos, with_labels=True, node_size=700, node_color=\"lightblue\")\n",
    "labels = nx.get_edge_attributes(W,'weight')\n",
    "nx.draw_networkx_edge_labels(W, pos, edge_labels=labels)\n",
    "plt.title(\"Weighted Graph\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjacency Matrix Representation\n",
    "Graphs can also be represented in matrix form using an adjacency matrix. The adjacency matrix of a graph is a square matrix where each element represents the presence (or absence) of an edge between two nodes.\n",
    "\n",
    "Let's represent the graph we just visualized as an adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Adjacency matrix representation of the graph\n",
    "adj_matrix = np.array([\n",
    "    [0, 1, 1, 0],  # Node 1 connections\n",
    "    [1, 0, 1, 0],  # Node 2 connections\n",
    "    [1, 1, 0, 1],  # Node 3 connections\n",
    "    [0, 0, 1, 0]   # Node 4 connections\n",
    "])\n",
    "\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output matrix corresponds to the graph we visualized, where `1` indicates the presence of an edge between nodes, and `0` indicates the absence of an edge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Message Passing in GNNs\n",
    "\n",
    "### 2.1 Basic Concept of Message Passing\n",
    "\n",
    "In Graph Neural Networks (GNNs), **message passing** is a fundamental process where nodes in a graph communicate with their neighbors to update their own states or features. This process is applied iteratively, allowing nodes to aggregate information from their local neighborhood, which is crucial for tasks like node classification, graph classification, and link prediction.\n",
    "\n",
    "#### Main assumptions:\n",
    "- Node labels can be presented as a function of the node's input features and the input features of its neighbors.\n",
    "- Contribution of each neighbor to the node's label can be summarized by a message function.\n",
    "- The node's label can be updated by aggregating the messages from its neighbors. Since the order of neighbors is arbitrary, the aggregation function should be permutation-invariant.\n",
    "- The node's label can be updated by combining the node's current label and the aggregated messages in an update function.\n",
    "\n",
    "### 2.2 Message Function\n",
    "\n",
    "The **message function** defines how information (messages) is transmitted from one node to another. The message function typically depends on the features of the source node and the edge connecting it to the target node.\n",
    "\n",
    "#### Example:\n",
    "For a node $v$, the message from its neighbor $u$ can be represented as:\n",
    "$$ m_{uv} = f(x_u, e_{uv}) $$\n",
    "where:\n",
    "- $x_u$ is the feature of node $u$\n",
    "- $e_{uv}$ is the feature of the edge connecting nodes $u$ and $v$\n",
    "- $f$ is a learnable function or a predefined function, such as a linear transformation or a neural network.\n",
    "\n",
    "### 2.3 Aggregation Function\n",
    "\n",
    "The **aggregation function** is responsible for combining all the incoming messages from the neighbors of a node. Common aggregation methods include summation, mean, and max pooling.\n",
    "\n",
    "#### Example:\n",
    "The aggregated message at node $v$ can be computed as:\n",
    "$$ a_v = \\text{AGG}(\\{m_{uv} : u \\in \\mathcal{N}(v)\\}) $$\n",
    "where:\n",
    "- $\\mathcal{N}(v)$ is the set of neighbors of node $v$\n",
    "- $\\text{AGG}$ is the aggregation function (e.g., sum, mean, max)\n",
    "\n",
    "### 2.4 Update Function\n",
    "\n",
    "The **update function** takes the aggregated messages and updates the state or features of the node. This step typically involves a learnable transformation, such as a neural network layer.\n",
    "\n",
    "#### Example:\n",
    "The updated state of node $v$ can be represented as:\n",
    "$$ x'_v = \\text{UPDATE}(x_v, a_v) $$\n",
    "where:\n",
    "- $x_v$ is the original feature of node $v$\n",
    "- $a_v$ is the aggregated message\n",
    "- $\\text{UPDATE}$ is a learnable function (e.g., a neural network layer) that combines the original node features with the aggregated information from its neighbors.\n",
    "\n",
    "### 2.5 Illustration of Message Passing\n",
    "\n",
    "Below is an illustration of the message passing process in a GNN. Each node in the graph receives messages from its neighbors, aggregates these messages, and updates its state.\n",
    "\n",
    "![Message Passing in GNN](https://ignnition.org/doc/_images/message_passing.png)\n",
    "\n",
    "In this illustration, nodes pass messages to their neighbors, which are then aggregated and used to update the node's own state. This process is repeated over multiple iterations, allowing each node to gather information from progressively more distant neighbors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Implementing Message Passing in Python\n",
    "Let’s implement a simple message passing step using Python and the networkx library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([(1, 2), (1, 3), (2, 3), (3, 4)])\n",
    "\n",
    "# Initialize node features (for simplicity, we use integers)\n",
    "node_features = {1: 1, 2: 2, 3: 3, 4: 4}\n",
    "\n",
    "# Message function:\n",
    "def message_function(u, v):\n",
    "    ### Define message function here\n",
    "    return \n",
    "\n",
    "# Aggregation function: sum of messages\n",
    "def aggregation_function(neighbors):\n",
    "    ### Define aggregation function here\n",
    "    return\n",
    "\n",
    "# Update function: \n",
    "def update_function(node, aggregated_message):\n",
    "    ### Define update function here\n",
    "    return\n",
    "\n",
    "\n",
    "# Perform message passing for each node\n",
    "new_node_features = {}\n",
    "for node in G.nodes():\n",
    "    # Get messages from neighbors\n",
    "    messages = [message_function(neighbor, node) for neighbor in G.neighbors(node)]\n",
    "    \n",
    "    # Aggregate messages\n",
    "    aggregated_message = aggregation_function(messages)\n",
    "    \n",
    "    # Update node feature (simple addition in this case)\n",
    "    new_node_features[node] = update_function(node, aggregated_message)\n",
    "\n",
    "print(\"Updated node features after message passing:\")\n",
    "print(new_node_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph Isomorphism and Weisfeiler-Lehman Test\n",
    "\n",
    "### 3.1 Graph Isomorphism Problem\n",
    "\n",
    "**Graph isomorphism** refers to the problem of determining whether two graphs are structurally identical. Two graphs are isomorphic if there exists a one-to-one correspondence between their nodes and edges such that adjacency is preserved.\n",
    "\n",
    "#### Example:\n",
    "- **Isomorphic Graphs:** Graphs with the same structure but potentially different node labels.\n",
    "- **Non-Isomorphic Graphs:** Graphs with different structures.\n",
    "\n",
    "### 3.2 Weisfeiler-Lehman Test\n",
    "\n",
    "The **Weisfeiler-Lehman (WL) test** is a graph isomorphism test that iteratively refines node labels based on the labels of neighboring nodes. This process helps to distinguish non-isomorphic graphs by capturing the structure of the graph more effectively.\n",
    "\n",
    "#### Algorithm:\n",
    "1. **Initialization:** Assign a unique label to each node based on its degree (or another initial feature).\n",
    "2. **Iteration:** At each step, update the label of each node based on the labels of its neighbors.\n",
    "3. **Termination:** Repeat until labels stabilize or a predefined number of iterations is reached.\n",
    "\n",
    "### 3.3 Illustration of Weisfeiler-Lehman Test\n",
    "\n",
    "Here’s an illustration showing the process of the Weisfeiler-Lehman test on two example graphs.\n",
    "\n",
    "![Weisfeiler-Lehman Test](https://miro.medium.com/v2/resize:fit:1400/0*SuYTb17bUhKzH-Ti)\n",
    "\n",
    "In this illustration, the WL test refines the node labels in each iteration, allowing the algorithm to distinguish between graphs with different structures.\n",
    "\n",
    "### 3.4 Implementing Weisfeiler-Lehman Test in Python\n",
    "\n",
    "Let’s implement a basic version of the Weisfeiler-Lehman test using Python. We’ll use the `networkx` library for graph manipulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Graphs for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two example graphs\n",
    "G1 = nx.Graph([(1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 1), (3, 6)])\n",
    "G2 = nx.Graph([(1, 2), (2, 3), (3, 1), (4, 5), (5, 6), (6, 4), (3, 6)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graphs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "nx.draw(G1, with_labels=True, node_size=700, node_color=\"lightblue\")\n",
    "plt.title(\"Graph 1\")\n",
    "plt.subplot(122)\n",
    "nx.draw(G2, with_labels=True, node_size=700, node_color=\"lightblue\")\n",
    "plt.title(\"Graph 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weisfeiler-Lehman Test Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "def weisfeiler_lehman(graph, iterations=2):\n",
    "    # Initialize node labels with their degrees\n",
    "    node_labels = {node: str(degree) for node, degree in graph.degree()}\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        new_labels = {}\n",
    "        # Create a mapping from node label and its neighbors' labels to a new label\n",
    "        label_map = defaultdict(lambda: defaultdict(int))\n",
    "        for node in graph.nodes():\n",
    "            neighbors_labels = [node_labels[neighbor] for neighbor in graph.neighbors(node)]\n",
    "            combined_label = (node_labels[node], tuple(sorted(neighbors_labels)))\n",
    "            new_label = hash(combined_label)  # Using hash as a simple unique new label\n",
    "            new_labels[node] = new_label\n",
    "        node_labels = new_labels\n",
    "    \n",
    "    return node_labels\n",
    "\n",
    "\n",
    "\n",
    "# Apply Weisfeiler-Lehman test\n",
    "labels_G1 = weisfeiler_lehman(G1)\n",
    "labels_G2 = weisfeiler_lehman(G2)\n",
    "\n",
    "print(\"Node labels for Graph G1:\")\n",
    "print(labels_G1)\n",
    "\n",
    "print(\"Node labels for Graph G2:\")\n",
    "print(labels_G2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the Node Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the nodes in the graphs\n",
    "nodes_labels = set(labels_G1.values()) | set(labels_G2.values())\n",
    "\n",
    "# create a mapping from node labels to colors\n",
    "label_to_color = {label: idx/len(nodes_labels) for idx, label in enumerate(nodes_labels)}\n",
    "\n",
    "# create a list of colors for each node in the graph\n",
    "node_colors_G1 = [label_to_color[label] for label in labels_G1.values()]\n",
    "node_colors_G2 = [label_to_color[label] for label in labels_G2.values()]\n",
    "\n",
    "# Visualize the graphs with node colors\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "nx.draw(G1, with_labels=True, node_size=700, node_color=node_colors_G1, cmap=plt.cm.plasma, vmin=0, vmax=1)\n",
    "plt.title(\"Graph 1\")\n",
    "plt.subplot(122)\n",
    "nx.draw(G2, with_labels=True, node_size=700, node_color=node_colors_G2, cmap=plt.cm.plasma, vmin=0, vmax=1)\n",
    "plt.title(\"Graph 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Limitations and Extensions\n",
    "The basic Weisfeiler-Lehman test has limitations, such as:\n",
    "  * **Limited Expressiveness:** The test may fail to distinguish some non-isomorphic graphs, particularly those with complex or symmetric structures.\n",
    "  \n",
    "  * **Extensions:** To address these limitations, higher-order Weisfeiler-Lehman tests and other advanced techniques can be used to increase the discriminative power of the test.\n",
    "  \n",
    "**Higher-Order WL Test:**\n",
    "\n",
    "An extension that refines the labels using higher-order neighborhoods, providing more discriminative power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Message-Passing Neural Networks (MPNN)\n",
    "\n",
    "### 4.1 Architecture Overview\n",
    "\n",
    "**Message-Passing Neural Networks (MPNNs)** are a class of Graph Neural Networks (GNNs) that operate by iteratively exchanging information (messages) between nodes in a graph. The core idea is that each node updates its feature vector by aggregating messages from its neighbors, allowing the network to capture complex dependencies and patterns in the graph structure.\n",
    "\n",
    "#### Key Components of MPNNs:\n",
    "\n",
    "1. **Message Function:** Determines how information is passed from one node to another.\n",
    "2. **Aggregation Function:** Combines incoming messages from neighboring nodes.\n",
    "3. **Update Function:** Updates the node's state or feature based on the aggregated messages.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `MessagePassing` in PyTorch Geometric\n",
    "\n",
    "`MessagePassing` is a base class provided by PyTorch Geometric for implementing message-passing neural networks. It abstracts the process of message passing, aggregation, and updating. Let’s understand how to use it:\n",
    "\n",
    "#### Key Methods in `MessagePassing`:\n",
    "\n",
    "1. **`forward()`**: Defines the forward pass of the layer. It usually takes `x` (node features) and `edge_index` (tensor containing the indices of source and target nodes for each edge).\n",
    "   \n",
    "2. **`message()`**: Computes messages to be sent along the edges. This method is called for each edge during the message-passing phase.\n",
    "\n",
    "3. **`aggregate()`**: Aggregates messages from neighboring nodes. Common aggregation methods are sum, mean, and max.\n",
    "\n",
    "4. **`update()`**: Updates the node embeddings after the aggregation of messages.\n",
    "\n",
    "5. **`propagate()`**: A helper function that combines `message()`, `aggregate()`, and `update()` functions. It is the main method called during the forward pass to execute the message-passing step.\n",
    "\n",
    "### Data Structures for Inputs\n",
    "\n",
    "- **`x` (Node Features)**: A tensor of shape `(num_nodes, num_features)` representing the features for each node.\n",
    "- **`edge_index`**: A tensor of shape `(2, num_edges)` where each column represents a directed edge from source to target.\n",
    "- **`edge_attr` (optional)**: A tensor of shape `(num_edges, num_edge_features)` representing the features for each edge.\n",
    "\n",
    "### Implementing an MPNN using `MessagePassing`\n",
    "\n",
    "Below is a detailed example of how to implement an MPNN layer using the `MessagePassing` class in PyTorch Geometric:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "\n",
    "\n",
    "# Define an MPNN model in PyTorch Geometric\n",
    "class Custom_MPNN(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Custom_MPNN, self).__init__()\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Add self-loops to the adjacency matrix to consider self-connections\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        \n",
    "        # Perform message passing\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    \n",
    "    def message(self, x_j):\n",
    "        # x_j: Input node features\n",
    "\n",
    "        # Perform linear transformation on node features\n",
    "        return \n",
    "    \n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        # Aggregates messages for each node \n",
    "\n",
    "        # Perform sum aggregation\n",
    "        return\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out: Aggregated messages\n",
    "        # x: Original node features\n",
    "\n",
    "        # Perform update operation on node features: simple addition\n",
    "        return \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Variants of MPNNs\n",
    "\n",
    "Several variants of MPNNs have been developed, each with unique characteristics and applications. Here are some of the most popular ones:\n",
    "\n",
    "#### 4.2.1 Graph Convolutional Networks (GCNs)\n",
    "\n",
    "**Graph Convolutional Networks (GCNs)** apply convolutional operations to graphs, allowing nodes to aggregate information from their neighbors based on the graph structure.\n",
    "\n",
    "- **GCN Formula:**\n",
    "  $$ H^{(l+1)} = \\text{ReLU} \\left( \\hat{A} H^{(l)} W^{(l)} \\right) $$\n",
    "  where:\n",
    "  - $\\hat{A}$ is the normalized adjacency matrix.\n",
    "  - $H^{(l)}$ is the node feature matrix at layer $l$.\n",
    "  - $W^{(l)}$ is the weight matrix at layer $l$.\n",
    "\n",
    "\n",
    "In terms of message, aggregation, and update functions, GCN can be described as follows:\n",
    "- **Message Function:** Linear transformation of node features:\n",
    "  $$ m^{(l+1)}_{uv} = \\frac{1}{\\sqrt{ N_u N_v}}W h^{(l)}_u $$\n",
    "   where $ N_u $ and $ N_v $ are the numbers of neighbors of nodes $u$ and $v$, respectively.\n",
    "  \n",
    "- **Aggregation Function:** Summing the messages from neighbors:\n",
    "  $$ a^{(l+1)}_v = \\sum_{u \\in \\mathcal{N}(v)} m^{(l+1)}_{uv} $$\n",
    "\n",
    "- **Update Function:** Applying a neural network layer (e.g., ReLU activation):\n",
    "  $$ h^{(l+1)}_v = \\text{ReLU}(a^{(l+1)}_v) $$\n",
    "\n",
    "#### Exercise: GCN Layer in pytorch geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "\n",
    "class GCNLayer(nn.MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNLayer, self).__init__(aggr='add')\n",
    "\n",
    "    # Implement the GCN layer \n",
    "\n",
    "# Example usage\n",
    "gcn_layer = GCNLayer(in_channels=3, out_channels=2)\n",
    "print(gcn_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Graph Attention Networks (GAT)\n",
    "\n",
    "**Graph Attention Networks (GAT)** incorporate an attention mechanism that allows the model to weigh the importance of different neighbors differently. This helps the network focus on more relevant parts of the graph.\n",
    "\n",
    "#### Key Features of GAT:\n",
    "- **Attention Mechanism:** Assigns different weights to different neighbors based on their relevance.\n",
    "- **Self-Attention:** Allows each node to attend to its neighbors with varying levels of importance.\n",
    "- **Scalability:** Handles graphs of varying sizes and densities efficiently.\n",
    " \n",
    " * **GAT Formula:** The core operation in GAT involves calculating attention coefficients and aggregating information from neighbors. The formula for updating a node's representation in GAT can be expressed as:\n",
    " \n",
    "$ H_i' = \\text{LeakyReLU} \\left( \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} W_v h_j \\right) $ where:\n",
    "* $H_i'$ is the updated feature vector for node $i$.\n",
    "* $\\alpha_{ij}$ is the attention coefficient between nodes $i$ and $j$.\n",
    "* $W$ is the weight matrix.\n",
    "* $\\mathcal{N}(i)$ denotes the set of neighbors of node $i$.\n",
    "\n",
    "\n",
    "- **Message Function:**  \n",
    "  In GAT, the message function computes the attention coefficient $\\alpha_{ij}$ for each pair of neighboring nodes $(i, j)$. This coefficient determines how much importance node $i$ should give to node $j$'s features. The message is then a combination of the attention coefficient and the transformed node features:\n",
    "\n",
    "  $$\n",
    "  m_{ij} = \\alpha_{ij} W_v h_j\n",
    "  $$\n",
    "\n",
    "  Here, $W_v$ is a learnable weight matrix that linearly transforms the feature vector $h_j$ of the neighbor node $j$. The attention coefficient $\\alpha_{ij}$ is computed using a self-attention mechanism:\n",
    "\n",
    "  $$\n",
    "  \\alpha_{ij} = \\frac{\\exp(\\text{LeakyReLU}( [W_q h_i || W_k h_j]))}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(\\text{LeakyReLU}( [W_q h_i || W_k h_k]))}\n",
    "  $$\n",
    "\n",
    "  where $W_k$ and $W_q$ are  learnable matrices that help compute the importance of edge $(i, j)$.\n",
    "\n",
    "- **Aggregation Function:**  \n",
    "  The aggregation function sums up all the weighted messages from the neighbors of node $i$:\n",
    "\n",
    "  $$\n",
    "  a_i = \\sum_{j \\in \\mathcal{N}(i)} m_{ij} = \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} W_v h_j\n",
    "  $$\n",
    "\n",
    "  This sum aggregates the messages (transformed features) from the neighbors of node $i$, weighted by the attention coefficients $\\alpha_{ij}$.\n",
    "\n",
    "- **Update Function:**  \n",
    "  The update function applies a non-linear transformation (e.g., LeakyReLU) to the aggregated messages to compute the updated node feature:\n",
    "\n",
    "  $$\n",
    "  h_i' = \\text{LeakyReLU}(a_i) \n",
    "  $$\n",
    "\n",
    "  This non-linearity allows the model to learn more complex patterns in the graph.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Exercise: GAT Layer in pytorch-geometric\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GATLayer, self).__init__(aggr='add')\n",
    "\n",
    "    # Implement the GAT layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 **GraphSAGE**\n",
    "\n",
    "**GraphSAGE** (Sample and Aggregation) is a variant of GNNs that scales well to large graphs by sampling a fixed-size neighborhood and aggregating information from these sampled neighbors. Unlike traditional GNNs that operate in a transductive setting (where all nodes are seen during training), GraphSAGE is designed to work in an inductive setting, meaning it can generalize to unseen nodes and graphs.\n",
    "\n",
    "#### Key Features of GraphSAGE:\n",
    "- **Inductive Learning:** Learns functions that can be applied to new nodes or graphs not seen during training.\n",
    "- **Sampling:** Samples a fixed-size neighborhood for each node, making it computationally efficient and scalable.\n",
    "- **Flexible Aggregation:** Supports different aggregation functions (mean, LSTM, pooling).\n",
    "\n",
    "#### GraphSAGE Formula:\n",
    "The formula for updating a node's representation in GraphSAGE can be expressed as:\n",
    "\n",
    "$$ H'_i = \\text{MLP} \\left( \\text{AGG} \\left( \\{H_j : j \\in \\mathcal{N}(i)\\} \\right) \\right) $$\n",
    "\n",
    "where:\n",
    "- $\\text{AGG}$ can be mean, LSTM, or pooling aggregation functions.\n",
    "- $\\text{MLP}$ is a multi-layer perceptron that combines the aggregated information.\n",
    "\n",
    "### GraphSAGE in Terms of Message, Aggregation, and Update Functions:\n",
    "\n",
    "- **Message Function:**  \n",
    "  In GraphSAGE, the message function computes messages from sampled neighboring nodes. Unlike other GNNs, GraphSAGE samples a fixed number of neighbors instead of using all neighbors, which helps in reducing computation for large graphs. The message is defined as the transformation of the neighbor's feature vectors:\n",
    "\n",
    "  $$\n",
    "  m_{ij} = h_j\n",
    "  $$\n",
    "\n",
    "  Here, $h_j$ represents the feature vector of the neighbor node $j$. The feature vectors are aggregated directly without additional transformations, as the transformation is applied after aggregation in GraphSAGE.\n",
    "\n",
    "- **Aggregation Function:**  \n",
    "  The aggregation function in GraphSAGE is flexible and can be one of several types, such as mean, LSTM, or pooling. The aggregation function computes the aggregated feature vector for node $i$ by combining the features of its sampled neighbors:\n",
    "\n",
    "  1. **Mean Aggregator:**\n",
    "     $$\n",
    "     a_i = \\frac{1}{|\\mathcal{N}(i)|} \\sum_{j \\in \\mathcal{N}(i)} h_j\n",
    "     $$\n",
    "\n",
    "     Here, the mean of the neighbors' features is taken.\n",
    "\n",
    "  2. **LSTM Aggregator:**\n",
    "     $$\n",
    "     a_i = \\text{LSTM}( \\{ h_j : j \\in \\mathcal{N}(i) \\} )\n",
    "     $$\n",
    "\n",
    "     In this case, the neighbors' features are passed through an LSTM to capture order-dependent information.\n",
    "\n",
    "  3. **Pooling Aggregator:**\n",
    "     $$\n",
    "     a_i = \\text{max}(\\{ \\text{ReLU}(W h_j + b) : j \\in \\mathcal{N}(i) \\})\n",
    "     $$\n",
    "\n",
    "     Here, each neighbor's features are passed through a fully connected layer with ReLU activation, followed by a max-pooling operation.\n",
    "\n",
    "- **Update Function:**  \n",
    "  The update function applies a Multi-Layer Perceptron (MLP) to the concatenation of the node's own features and the aggregated features. This allows the node to incorporate both its own information and the information from its neighbors:\n",
    "\n",
    "  $$\n",
    "  h_i' = \\text{MLP} \\left( \\text{concat}(h_i, a_i) \\right)\n",
    "  $$\n",
    "\n",
    "  Here, the MLP consists of fully connected layers, and $\\text{concat}(h_i, a_i)$ denotes the concatenation of the node's current feature vector $h_i$ with the aggregated neighbor features $a_i$. This enables the model to learn more complex combinations of features.\n",
    "\n",
    "\n",
    "**Example: GraphSAGE Layer in Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGELayer(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GraphSAGELayer, self).__init__(aggr='add')\n",
    "\n",
    "    # Implement the GraphSAGE layer\n",
    "\n",
    "# Example usage\n",
    "sage_layer = GraphSAGELayer(in_channels=3, out_channels=2)\n",
    "print(sage_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training and Optimization\n",
    "\n",
    "In this exercise, you will learn how to train Graph Neural Networks (GNNs) using PyTorch Geometric on a standard graph dataset. The goal is to load a dataset, split it into training and test sets, choose one of the graph architectures (GCN, GAT, GraphSAGE), define a network, and train it to make predictions on the graph data.\n",
    "\n",
    "### Task:\n",
    "\n",
    "1. **Choose a Dataset**: We will use the **Cora** dataset, which is a common benchmark for GNNs. It is a citation network where nodes represent documents, and edges represent citations between them.\n",
    "\n",
    "2. **Split the Data**: Split the dataset into training, validation, and test sets.\n",
    "\n",
    "3. **Choose a Graph Architecture**: Choose one of the predefined architectures (e.g., GCN, GAT, GraphSAGE).\n",
    "\n",
    "4. **Define the Network**: Initialize the network with the chosen architecture.\n",
    "\n",
    "5. **Train the Network**: Train the network on the training set and evaluate its performance on the test set.\n",
    "\n",
    "### 4.3.1 Code Drafts\n",
    "\n",
    "Below are the draft cells to guide you through each step:\n",
    "\n",
    "#### Step 1: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora', transform=NormalizeFeatures())\n",
    "data = dataset[0]  # Cora dataset has only one graph\n",
    "\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Number of features: {data.num_node_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Split the Data\n",
    "PyTorch Geometric automatically handles data splits for Planetoid datasets (Cora, Citeseer, Pubmed). The dataset has built-in masks for training, validation, and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, validation, and test masks\n",
    "train_mask = data.train_mask\n",
    "val_mask = data.val_mask\n",
    "test_mask = data.test_mask\n",
    "\n",
    "print(f'Number of training nodes: {train_mask.sum().item()}')\n",
    "print(f'Number of validation nodes: {val_mask.sum().item()}')\n",
    "print(f'Number of test nodes: {test_mask.sum().item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Choose a Graph Architecture\n",
    "Choose one of the graph architectures (e.g., GCN, GAT, GraphSAGE) that you have defined above. For this example, we can use Graph Convolutional Networks (GCN). You can use any other architecture and number of layers as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCNNet, self).__init__()\n",
    "        self.conv1 = GCNLayer(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNLayer(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First GCN layer + ReLU activation\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # Second GCN layer + LogSoftmax activation for multi-class classification\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "# Initialize the GCN model\n",
    "model = GCNNet(in_channels=dataset.num_node_features, hidden_channels=16, out_channels=dataset.num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Train the Network\n",
    "Define the training function and train the network using the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    out = model(data.x, data.edge_index)  # Forward pass\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])  # Compute the loss\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update model parameters\n",
    "    return loss.item()\n",
    "\n",
    "# Store the training loss values\n",
    "loss_values = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    loss_values.append(loss)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Evaluate the Network\n",
    "Evaluate the trained model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)  # Forward pass\n",
    "    pred = out.argmax(dim=1)  # Use the class with the highest probability\n",
    "    correct = (pred[test_mask] == data.y[test_mask]).sum()  # Check how many predictions match the true labels\n",
    "    acc = int(correct) / int(test_mask.sum())  # Calculate accuracy\n",
    "    return acc\n",
    "\n",
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Plot the Learning Curve\n",
    "To visualize the learning process, plot the training loss over epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(loss_values) + 1), loss_values, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Approaches to the Isomorphism Problem in GNNs\n",
    "\n",
    "### 5.1 Expressive Power of GNNs\n",
    "\n",
    "The **expressive power** of Graph Neural Networks (GNNs) refers to their ability to distinguish between different graph structures. While GNNs have shown great promise in learning from graph-structured data, they have inherent limitations when it comes to distinguishing non-isomorphic graphs—graphs that have the same structure but are labeled differently.\n",
    "\n",
    "#### Key Points:\n",
    "- **Graph Isomorphism:** Two graphs are isomorphic if there exists a one-to-one correspondence between their nodes and edges such that adjacency is preserved.\n",
    "- **Expressive Limitations of GNNs:** Standard GNNs, such as those based on message passing, may fail to distinguish between certain non-isomorphic graphs. This limitation arises because the node embeddings may converge to similar representations, even if the underlying graph structures are different.\n",
    "\n",
    "#### Example:\n",
    "Consider two non-isomorphic graphs that have the same node degree distribution. A simple GNN might produce identical node embeddings for both graphs, failing to recognize that they are actually different.\n",
    "\n",
    "### 5.2 Advanced Techniques to Address Graph Isomorphism\n",
    "\n",
    "To overcome the limitations of standard GNNs in distinguishing non-isomorphic graphs, several advanced techniques and specialized architectures have been developed. These techniques enhance the expressive power of GNNs and provide better solutions to the graph isomorphism problem.\n",
    "\n",
    "#### 5.2.1 Graph Isomorphism Network (GIN)\n",
    "\n",
    "**Graph Isomorphism Network (GIN)** is a type of GNN designed to have a stronger expressive power, making it as powerful as the Weisfeiler-Lehman (WL) test in distinguishing non-isomorphic graphs.\n",
    "\n",
    "- **Key Idea:** GIN uses an aggregation function that mimics the WL test, allowing it to distinguish graphs that other GNNs might not.\n",
    "- **Architecture:** GINs typically use a sum aggregation function combined with a multi-layer perceptron (MLP) for node updates, which enhances their ability to capture graph structure.\n",
    "\n",
    "#### GIN Formula:\n",
    "$$ h_v^{(l+1)} = \\text{MLP}^{(l)} \\left( \\left(1 + \\epsilon^{(l)} \\right) \\cdot h_v^{(l)} + \\sum_{u \\in \\mathcal{N}(v)} h_u^{(l)} \\right) $$\n",
    "where:\n",
    "- $h_v^{(l)}$ is the feature of node $v$ at layer $l$.\n",
    "- $\\epsilon^{(l)}$ is a learnable parameter.\n",
    "- $\\text{MLP}^{(l)}$ is a multi-layer perceptron.\n",
    "\n",
    "\n",
    "#### Example: GIN Layer in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "class GINLayer(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, eps=0.0):\n",
    "        super(GINLayer, self).__init__()\n",
    "        # Implement GIN layer\n",
    "\n",
    "# Example usage\n",
    "gin_layer = GINLayer(in_channels=3, out_channels=2)\n",
    "print(gin_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Cell-Weisfeiler-Lehman Networks (CW Networks)\n",
    "\n",
    "**Cell-Weisfeiler-Lehman Networks (CW Networks)** are an extension of the Weisfeiler-Lehman (WL) test designed to operate on cell complexes rather than just graphs. This version simplifies the implementation by focusing on nodes (0-cells) and edges (1-cells) only.\n",
    "\n",
    "\n",
    "#### Illustration:\n",
    "![The CWL colouring procedure for the yellow edge of the cell complex.](https://d3i71xaburhd42.cloudfront.net/093bd7a7dd31a8eb4da371d012da5fd272bb96ca/4-Figure4-1.png)\n",
    "\n",
    "##### Key Concepts:\n",
    "- **Cell Complexes:** These are higher-dimensional generalizations of graphs. While graphs consist of nodes (0-cells) and edges (1-cells), cell complexes can include higher-dimensional cells like faces (2-cells) and volumes (3-cells).\n",
    "- **Message Passing on Cells:** In CW Networks, message passing extends to higher-dimensional cells, allowing the network to aggregate information from more complex structures.\n",
    "\n",
    "\n",
    "##### Simplified CW Layer Implementation\n",
    "\n",
    "Below is a simplified implementation of a CW Network that focuses on nodes and edges:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import scatter\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "class SimplifiedCWLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SimplifiedCWLayer, self).__init__(aggr='add')  # Define aggregation method\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels  \n",
    "        self.W_node_to_edge = Parameter(torch.randn(in_channels, out_channels))\n",
    "        self.W_edge_to_node = Parameter(torch.randn(in_channels, out_channels))\n",
    "        self.W_node_to_neigh = Parameter(torch.randn(in_channels, out_channels))\n",
    "        self.W_node_to_itself = Parameter(torch.randn(in_channels, out_channels))\n",
    "        self.W_edge_to_itself = Parameter(torch.randn(in_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: Node features\n",
    "        # edge_index: Edge indices\n",
    "        # edge_attr: Edge features\n",
    "\n",
    "        edge_message = # Implement edge message computation\n",
    "        edge_aggr = # Implement edge aggregation\n",
    "        edge_update = # Implement edge update\n",
    "        node_update = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "        return node_update, edge_update\n",
    "    \n",
    "    def message(self, x_j, edge_attr):\n",
    "        # x_j: Input node features\n",
    "        # edge_attr: Input edge features\n",
    "\n",
    "        node_message = # Implement node component computation\n",
    "        node_message += # Implement edge component computation\n",
    "\n",
    "\n",
    "\n",
    "        return node_message\n",
    "    \n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out: Aggregated messages\n",
    "        # x: Original node features\n",
    "\n",
    "        output = # Implement update function\n",
    "        return output\n",
    "    \n",
    "    \n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(3, 3)  # 3 nodes, 3 features per node\n",
    "edge_index = torch.tensor([[0, 1], [1, 2]], dtype=torch.long).t().contiguous()  # Edges in COO format\n",
    "edge_attr = torch.randn(2, 3)  # 2 edges, 3 features per edge\n",
    "\n",
    "cw_layer = SimplifiedCWLayer(in_channels=3, out_channels=2)\n",
    "x, edge_attr = cw_layer(x, edge_index, edge_attr)\n",
    "\n",
    "print(\"Updated Node Features:\", x)\n",
    "print(\"Updated Edge Features:\", edge_attr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the Code:\n",
    "\n",
    "* **Node and Edge Features:** The code focuses on nodes and edges.\n",
    "\n",
    "* **Message Passing:** The SimplifiedCWLayer class uses message, update, and aggregate functions to update node and edge features.\n",
    "\n",
    "* **PyTorch Geometric Integration:** Utilizes `MessagePassing` for streamlined GNN operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Training and Evaluating a GNN\n",
    "In this final exercise, you will:\n",
    "\n",
    "1. **Select a Graph Dataset:** Choose a dataset from `PyTorch Geometric`, such as `Cora`, `Citeseer`, or `Pubmed`.\n",
    "2. **Choose a GNN Layer:** Select one of the layers implemented above, either `GINLayer` or `SimplifiedCWLayer`.\n",
    "3. **Build a Network:** Create a GNN using the selected layer.\n",
    "4. **Train the Network:** Train the network on the training dataset, plot the training loss curve, and evaluate its performance on the test dataset.\n",
    "\n",
    "\n",
    "#### Code Draft:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Step 1: Load Dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "\n",
    "# Step 2: Define Network\n",
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = SimplifiedCWLayer(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x, _ = self.conv1(x, edge_index, data.edge_attr)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Step 3: Initialize Model\n",
    "model = GNNModel(in_channels=dataset.num_node_features, out_channels=dataset.num_classes)\n",
    "\n",
    "# Step 4: Training Loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(dataset[0])\n",
    "    loss = F.nll_loss(out[dataset[0].train_mask], dataset[0].y[dataset[0].train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "# Step 5: Plot Training Curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Evaluate Model\n",
    "model.eval()\n",
    "_, pred = model(dataset[0]).max(dim=1)\n",
    "correct = (pred[dataset[0].test_mask] == dataset[0].y[dataset[0].test_mask]).sum()\n",
    "acc = int(correct) / int(dataset[0].test_mask.sum())\n",
    "print(f'Test Accuracy: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks:\n",
    "1. **Experiment with Different Architectures:** Use either `GINLayer` or `SimplifiedCWLayer`.\n",
    "2. **Modify Hyperparameters:** Change learning rates, epochs, and architectures.\n",
    "3. **Analyze Results:** Observe and discuss the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we covered the foundational concepts of Graph Neural Networks (GNNs), delving into graph theory, message passing, and advanced techniques for addressing the graph isomorphism problem. We explored different types of GNN architectures, such as Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), GraphSAGE, and more advanced methods like Graph Isomorphism Networks (GIN) and Cell-Weisfeiler-Lehman Networks (CW Networks). Each architecture enhances the expressive power of GNNs in different ways, making them more robust for various graph-based tasks.\n",
    "\n",
    "Understanding these foundational concepts is crucial for leveraging GNNs in real-world applications such as social network analysis, molecular graph generation, knowledge graph completion, and many more. As we have seen, the choice of architecture and technique depends on the nature of the problem and the underlying assumptions about the graph data. The advanced techniques and architectures introduced here provide a solid foundation for tackling complex graph learning problems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
